{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989a88b6-6b9d-4dd7-a5b3-b2bc1d83af4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd95532-89f0-4598-9ae3-65be85af7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3398, dev:486, test:970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "df = df[df['label_vector'] != 'none']\n",
    "\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Fine-grained Vector of Sexism: for posts which are sexist, classify them into one of 11 categories:\n",
    "1.1 threats of harm\n",
    "1.2 incitement and encouragement of harm\n",
    "2.1 descriptive attacks\n",
    "2.2 aggressive and emotive attacks\n",
    "2.3 dehumanising attacks & overt sexual objectification\n",
    "3.1 casual use of gendered slurs, profanities, and insults\n",
    "3.2 immutable gender differences and gender stereotypes\n",
    "3.3 backhanded gendered compliments\n",
    "3.4 condescending explanations or unwelcome advice\n",
    "4.1 supporting mistreatment of individual women\n",
    "4.2 supporting systemic discrimination against women as a group\n",
    "\n",
    "Given a post, determine which class it belongs to.\n",
    "\n",
    "### Post: \n",
    "{POST}\n",
    "### Class: \"\"\"\n",
    "\n",
    "column='label_vector'\n",
    "llm_path = \"task_c_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe9f0f8-2e31-4e32-82c0-394a5e30bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. imports\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, set_seed\n",
    "\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c782b2-f4d5-4dd2-af9d-ae646a13f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The arguments for the DPO training script.\n",
    "    \"\"\"\n",
    "\n",
    "    # data parameters\n",
    "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
    "\n",
    "    # training parameters\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=f\"{llm_path}/\",\n",
    "        metadata={\"help\": \"the location of the SFT model name or path\"},\n",
    "    )\n",
    "    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"optimizer learning rate\"})\n",
    "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
    "    warmup_steps: Optional[int] = field(default=200, metadata={\"help\": \"the number of warmup steps\"})\n",
    "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
    "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
    "\n",
    "    per_device_train_batch_size: Optional[int] = field(default=16, metadata={\"help\": \"train batch size per device\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"eval batch size per device\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    gradient_checkpointing_use_reentrant: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": \"whether to use reentrant for gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
    "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
    "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
    "\n",
    "    max_prompt_length: Optional[int] = field(default=512, metadata={\"help\": \"the maximum prompt length\"})\n",
    "    max_length: Optional[int] = field(default=600, metadata={\"help\": \"the maximum sequence length\"})\n",
    "    max_steps: Optional[int] = field(default=2000, metadata={\"help\": \"max number of training steps\"})\n",
    "    logging_steps: Optional[int] = field(default=50, metadata={\"help\": \"the logging frequency\"})\n",
    "    save_steps: Optional[int] = field(default=100, metadata={\"help\": \"the saving frequency\"})\n",
    "    eval_steps: Optional[int] = field(default=100, metadata={\"help\": \"the evaluation frequency\"})\n",
    "\n",
    "    output_dir: Optional[str] = field(default=f\"{llm_path}_rlhf\", metadata={\"help\": \"the output directory\"})\n",
    "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=True, metadata={\"help\": \"whether to load the model in 4bit\"})\n",
    "    model_dtype: Optional[str] = field(\n",
    "        default=\"float16\", metadata={\"help\": \"model_dtype[float16, bfloat16, float] for loading.\"}\n",
    "    )\n",
    "\n",
    "    # instrumentation\n",
    "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
    "    report_to: Optional[str] = field(\n",
    "        default=\"tensorboard\",\n",
    "        metadata={\n",
    "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
    "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
    "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
    "        },\n",
    "    )\n",
    "    # debug argument for distributed training\n",
    "    ignore_bias_buffers: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
    "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
    "        },\n",
    "    )\n",
    "    seed: Optional[int] = field(\n",
    "        default=0, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"}\n",
    "    )\n",
    "    # f: Optional[str] = field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6cc928-6d36-4be9-abb0-ab6b1967c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_exchange_paired(\n",
    "    df: pd,\n",
    "    column: str,\n",
    "    num_proc=24,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts are structured as follows:\n",
    "      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n",
    "    \"\"\"\n",
    "    \n",
    "    def return_prompt_and_responses(samples) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"prompt\": samples[\"question\"],\n",
    "            \"chosen\": samples[\"response_j\"],\n",
    "            \"rejected\": samples[\"response_k\"],\n",
    "        }\n",
    "    labels = df[column].tolist()\n",
    "    labels_set = list(set(labels))\n",
    "    posts = df['text'].tolist()\n",
    "    dataset_lists = {'question':[], 'response_j':[], 'response_k':[]}\n",
    "    for post, label in zip(posts, labels):\n",
    "        \n",
    "        \n",
    "        for index in range(len(labels_set)):\n",
    "            if labels_set[index] != label:\n",
    "                dataset_lists['question'] += [prompt_template.replace(\"{POST}\", post)]\n",
    "                dataset_lists['response_j'] += [label]\n",
    "                dataset_lists['response_k'] += [labels_set[index]]\n",
    "        # dataset_lists.append(dataset_list)\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_lists)\n",
    "    \n",
    "    return dataset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=['question', 'response_j', 'response_k'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cc8080-d5fa-452a-98b2-0f3012dcb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptArguments\n",
    "parser = HfArgumentParser((ScriptArguments,) )\n",
    "# parser.add_parser(\"-f\")\n",
    "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "\n",
    "set_seed(script_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ac9eba-a916-4a2b-a5cb-6a6300731b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe51dfadfabc414aaea7171cf6835f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "torch_dtype = torch.float\n",
    "if script_args.model_dtype == \"float16\":\n",
    "    torch_dtype = torch.float16\n",
    "elif script_args.model_dtype == \"bfloat16\":\n",
    "    torch_dtype = torch.bfloat16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    load_in_4bit=script_args.load_in_4bit,\n",
    "    device_map={\"\": Accelerator().local_process_index},\n",
    "    token=\"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\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "if script_args.ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path, token=\"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\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9374d0c5-af5c-4845-9e0f-3d9980ebebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a79ef6f06b4297838920b21abe76b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/33980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc06c7c8354646fca5368a1be7a2b778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/4860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Load the Stack-exchange paired dataset\n",
    "train_dataset = get_stack_exchange_paired(df=train_df, column=column)\n",
    "\n",
    "# 3. Load evaluation dataset\n",
    "eval_dataset = get_stack_exchange_paired(df=dev_df, column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fb9269-692a-4ac9-a07c-f6c74a94a74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 33980\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bea6ed-e42d-4007-b28c-e514bf38fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize training arguments:\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    max_steps=script_args.max_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    # do_eval=True,\n",
    "    # eval_strategy=\"steps\",\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    output_dir=script_args.output_dir,\n",
    "    report_to=script_args.report_to,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    warmup_steps=script_args.warmup_steps,\n",
    "    optim=script_args.optimizer_type,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"dpo_llama2\",\n",
    "    gradient_checkpointing_kwargs=dict(use_reentrant=script_args.gradient_checkpointing_use_reentrant),\n",
    "    seed=script_args.seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61a7e79-0ae0-4d22-8e6a-61e7e149654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=script_args.lora_r,\n",
    "    lora_alpha=script_args.lora_alpha,\n",
    "    lora_dropout=script_args.lora_dropout,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"out_proj\",\n",
    "        \"fc_in\",\n",
    "        \"fc_out\",\n",
    "        \"wte\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67afb17f-c8f2-400c-8db6-f07fc463ed6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1019ae60edd64a3396a2865faa3a0132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91efde05f8e47d3afc96333f669f050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# 5. initialize the DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=script_args.beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    max_prompt_length=script_args.max_prompt_length,\n",
    "    max_length=script_args.max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caabd46-9cee-4621-8f4f-5376428faf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1936' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1936/2000 4:12:12 < 08:20, 0.13 it/s, Epoch 3.64/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(script_args.output_dir)\n",
    "\n",
    "# output_dir = os.path.join(script_args.output_dir, \"final_checkpoint\")\n",
    "# dpo_trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d4d15-6dff-4245-a220-6d505fcc5e1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bde2a1-747d-4560-9207-9c3524646a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3398, dev:486, test:970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "df = df[df['label_category'] != 'none']\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Fine-grained Vector of Sexism: for posts which are sexist, classify them into one of 11 categories:\n",
    "1.1 threats of harm\n",
    "1.2 incitement and encouragement of harm\n",
    "2.1 descriptive attacks\n",
    "2.2 aggressive and emotive attacks\n",
    "2.3 dehumanising attacks & overt sexual objectification\n",
    "3.1 casual use of gendered slurs, profanities, and insults\n",
    "3.2 immutable gender differences and gender stereotypes\n",
    "3.3 backhanded gendered compliments\n",
    "3.4 condescending explanations or unwelcome advice\n",
    "4.1 supporting mistreatment of individual women\n",
    "4.2 supporting systemic discrimination against women as a group\n",
    "\n",
    "Given a post, determine which class it belongs to.\n",
    "\n",
    "### Post: \n",
    "{POST}\n",
    "### Class: \"\"\"\n",
    "\n",
    "column='label_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd829bbe-1bc6-49a7-987a-adb219ee324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805eaad0f8d04584a6575383da37268b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_path = \"task_c_llm_rlhf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, padding_side='left')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=quant_config, device_map={\"\": 0})\n",
    "# finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f6e416-a8e8-4dda-bf4e-f0d49b4efe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EDOSDataset(Dataset):\n",
    "    def __init__(self, df, prompt_template, column):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df[column].tolist()\n",
    "        self.prompt_template=prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        inputs, inputs_labels = [], []\n",
    "        for idx in idxs:\n",
    "            \n",
    "            inputs.append(self.prompt_template.replace(\"{POST}\", self.texts[idx]))\n",
    "            inputs_labels.append(self.labels[idx])\n",
    "        \n",
    "        return {\"inputs\":inputs, \"labels\": inputs_labels}\n",
    "    \n",
    "def make_the_generations(model, tokenizer, data_loader):\n",
    "    gen_texts, labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        input_data = batch['inputs']\n",
    "        labels += batch['labels']\n",
    "        tokenized_input_data = tokenizer(input_data, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        # print(tokenized_input_data)\n",
    "        outputs = finetuned_model.generate(\n",
    "            **tokenized_input_data,\n",
    "            pad_token_id= tokenizer.eos_token_id,\n",
    "            max_new_tokens=20,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(outputs[idx], skip_special_tokens=True)[len(input_data[idx]):]\n",
    "                          for idx in range(len(outputs))]\n",
    "        gen_texts += generated_texts\n",
    "    return gen_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005f83ef-3e53-4b2e-a5a3-9ddaef20839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [04:16<00:00,  4.75s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = EDOSDataset(df=train_df, prompt_template=prompt_template, column=column)\n",
    "train_dataloader =  DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "train_texts, train_labels = make_the_generations(finetuned_model, tokenizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628c9715-13df-4f95-b208-640ba3ec1e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['3.3 backhanded gendered compliments!',\n",
       "  '1. dehumanising, overtly sexual objectification, and boasting about exploiting women. 2',\n",
       "  '2.3 dehumanising attacks & overt deification (comparing her to a pig) �',\n",
       "  \".2 supporting systemic discrimination against women as a group (linking welfare to a woman's rights\",\n",
       "  '1.2 encouragement and incitement to harm (or, in this case, harm, in',\n",
       "  '1.2 encouraging and enabling harm (or, at the very least, not condemning or intervening)'],\n",
       " ['3.3 backhanded gendered compliments',\n",
       "  '2.3 dehumanising attacks & overt sexual objectification',\n",
       "  '2.3 dehumanising attacks & overt sexual objectification',\n",
       "  '4.2 supporting systemic discrimination against women as a group',\n",
       "  '1.2 incitement and encouragement of harm',\n",
       "  '4.1 supporting mistreatment of individual women'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:6], train_labels[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d5c02d-6719-49a4-84d6-a9abedd32070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:37<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_data = EDOSDataset(df=dev_df, prompt_template=prompt_template, column=column)\n",
    "dev_dataloader =  DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "dev_texts, dev_labels = make_the_generations(finetuned_model, tokenizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6d7e62-943a-480d-96a0-f66e43d34219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:03<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "test_data = EDOSDataset(df=test_df, prompt_template=prompt_template, column=column)\n",
    "test_dataloader =  DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "test_texts, test_labels = make_the_generations(finetuned_model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28079fd-bf29-46af-a5b1-52569ea16374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.2 incitement and encouragement of harm',\n",
       " '2.1 descriptive attacks',\n",
       " '3.1 casual use of gendered slurs, profanities, and insults',\n",
       " '2.1 descriptive attacks',\n",
       " '2.2 aggressive and emotive attacks']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b927be-172a-46b0-9123-0eba974b0dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.2 planning and consideration (of a specific, bizarre, and potentially creepy scenario) and a',\n",
       " '3.2 dehumanising, degrading, and objectifying desire for a specific body part (',\n",
       " '2.1 descriptive curiosity (unfamiliarity and suspicion, and a tinge of criticism)',\n",
       " '4.2: degrading, violent, and overtly sexualised violence (and a very aggressive',\n",
       " '1. Descriptive attacks (general claim) - a negative description of women as a group.',\n",
       " '3.1 (using a gendered metaphor, in this case, \"pussy\" as a',\n",
       " \"4.2 supporting legal clarification of sexual violence\\n\\n### Post: \\nI'm not a fan of\",\n",
       " '4.2 supporting systemic discrimination against women as a group (Pussy pass) - unfair advantage in',\n",
       " '4.2 supporting systemic discrimination against women as a group (protection from punishment due to systemic bias',\n",
       " '3.3 immutable gender differences and gender stereotypes (and norms)',\n",
       " '3.1 casual use of gendered slurs, profanities, and insults, and a',\n",
       " '3.2, using a male perspective (resilience and strength) to diffuse the situation and',\n",
       " \"2.2, Immoral (in a way, as it's a contradictory and unappealing\",\n",
       " '2.2 aggressive and emotive attacks',\n",
       " '2.2 aggressive and emotive attacks',\n",
       " '1.2 incitement (wishing for harm) not necessarily to be carried out by oneself',\n",
       " '3.2 immutable gender differences and gender stereotypes (AWALT - \"As Women (generally)',\n",
       " '3.3 casual use of gendered slurs, profanities, and insults, and a',\n",
       " '2.1 descriptive negative opinions about women as a group (irrational, unattractive,',\n",
       " '1.2 incited to incitement (and incited to action, to report the post']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cb43b5dd-6d8b-47b1-8103-815cea88097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {color: black;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              5),\n",
       "                                                                 sublinear_tf=True,\n",
       "                                                                 tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;)),\n",
       "                                                (&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(2,\n",
       "                                                                              5),\n",
       "                                                                 tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-137\" type=\"checkbox\" ><label for=\"sk-estimator-id-137\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              5),\n",
       "                                                                 sublinear_tf=True,\n",
       "                                                                 tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;)),\n",
       "                                                (&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(2,\n",
       "                                                                              5),\n",
       "                                                                 tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-138\" type=\"checkbox\" ><label for=\"sk-estimator-id-138\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vectorizer: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;,\n",
       "                                TfidfVectorizer(ngram_range=(1, 5),\n",
       "                                                sublinear_tf=True,\n",
       "                                                tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;)),\n",
       "                               (&#x27;count-vec&#x27;,\n",
       "                                CountVectorizer(ngram_range=(2, 5),\n",
       "                                                tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tfidf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-139\" type=\"checkbox\" ><label for=\"sk-estimator-id-139\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 5), sublinear_tf=True,\n",
       "                tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>count-vec</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-140\" type=\"checkbox\" ><label for=\"sk-estimator-id-140\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(2, 5),\n",
       "                tokenizer=&lt;function my_tokenizer at 0x7fb042f563a0&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-141\" type=\"checkbox\" ><label for=\"sk-estimator-id-141\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Vectorizer',\n",
       "                 FeatureUnion(transformer_list=[('tfidf',\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              5),\n",
       "                                                                 sublinear_tf=True,\n",
       "                                                                 tokenizer=<function my_tokenizer at 0x7fb042f563a0>)),\n",
       "                                                ('count-vec',\n",
       "                                                 CountVectorizer(ngram_range=(2,\n",
       "                                                                              5),\n",
       "                                                                 tokenizer=<function my_tokenizer at 0x7fb042f563a0>))])),\n",
       "                ('Classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV, ElasticNetCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    return doc.lower().split(' ')\n",
    "\n",
    "vectorizer_1 = CountVectorizer(ngram_range=(2,5), lowercase=True, tokenizer=my_tokenizer)\n",
    "vectorizer_2 = TfidfVectorizer(ngram_range=(1,5), \n",
    "                                     lowercase=True, \n",
    "                                     sublinear_tf=True, \n",
    "                                     use_idf=True, tokenizer=my_tokenizer)\n",
    "features = FeatureUnion([\n",
    "    \n",
    "    (\"tfidf\", vectorizer_2),\n",
    "    (\"count-vec\", vectorizer_1),\n",
    "])\n",
    "\n",
    "class_mapper = Pipeline (\n",
    "    steps=[\n",
    "        (\"Vectorizer\", features),\n",
    "        # (\"TruncatedSVD\", TruncatedSVD(n_components=600)),\n",
    "        ('Classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# class_mapper.fit(train_texts+train_labels+train_df['text'].tolist(), train_labels+train_labels+train_labels)\n",
    "# class_mapper.fit(train_labels, train_labels)\n",
    "# class_mapper.fit(train_texts+train_df['text'].tolist(), \n",
    "#                  train_labels+train_df[column].tolist())\n",
    "X = [x + \"\\n\" + gen for gen, x in zip(train_texts, train_df['text'].tolist())]\n",
    "\n",
    "class_mapper.fit(X, train_labels)\n",
    "# +train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6ac7891-13ea-403b-9fc4-5bdd2f3bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     1.0000    0.6607    0.7957        56\n",
      "                       1.2 incitement and encouragement of harm     0.9468    0.9803    0.9632       254\n",
      "                                        2.1 descriptive attacks     0.8908    0.9554    0.9219       717\n",
      "                             2.2 aggressive and emotive attacks     0.9153    0.9316    0.9234       673\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.9124    0.9900    0.9496       200\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.9320    0.9042    0.9179       637\n",
      "        3.2 immutable gender differences and gender stereotypes     0.9268    0.9113    0.9190       417\n",
      "                            3.3 backhanded gendered compliments     1.0000    0.9219    0.9593        64\n",
      "             3.4 condescending explanations or unwelcome advice     1.0000    0.7021    0.8250        47\n",
      "                4.1 supporting mistreatment of individual women     0.9828    0.7600    0.8571        75\n",
      "4.2 supporting systemic discrimination against women as a group     0.9719    0.9380    0.9546       258\n",
      "\n",
      "                                                       accuracy                         0.9250      3398\n",
      "                                                      macro avg     0.9526    0.8778    0.9079      3398\n",
      "                                                   weighted avg     0.9268    0.9250    0.9242      3398\n",
      "\n",
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.5000    0.2500    0.3333         8\n",
      "                       1.2 incitement and encouragement of harm     0.6591    0.8056    0.7250        36\n",
      "                                        2.1 descriptive attacks     0.4750    0.7451    0.5802       102\n",
      "                             2.2 aggressive and emotive attacks     0.6447    0.5104    0.5698        96\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4118    0.4828    0.4444        29\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.6627    0.6044    0.6322        91\n",
      "        3.2 immutable gender differences and gender stereotypes     0.5849    0.5167    0.5487        60\n",
      "                            3.3 backhanded gendered compliments     0.5000    0.2222    0.3077         9\n",
      "             3.4 condescending explanations or unwelcome advice     0.0000    0.0000    0.0000         7\n",
      "                4.1 supporting mistreatment of individual women     1.0000    0.0909    0.1667        11\n",
      "4.2 supporting systemic discrimination against women as a group     0.6296    0.4595    0.5312        37\n",
      "\n",
      "                                                       accuracy                         0.5679       486\n",
      "                                                      macro avg     0.5516    0.4261    0.4399       486\n",
      "                                                   weighted avg     0.5848    0.5679    0.5560       486\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.5000    0.1250    0.2000        16\n",
      "                       1.2 incitement and encouragement of harm     0.6486    0.6575    0.6531        73\n",
      "                                        2.1 descriptive attacks     0.4525    0.6732    0.5412       205\n",
      "                             2.2 aggressive and emotive attacks     0.5575    0.5052    0.5301       192\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4211    0.4211    0.4211        57\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.6467    0.6538    0.6503       182\n",
      "        3.2 immutable gender differences and gender stereotypes     0.4554    0.3866    0.4182       119\n",
      "                            3.3 backhanded gendered compliments     0.8000    0.2222    0.3478        18\n",
      "             3.4 condescending explanations or unwelcome advice     0.5000    0.0714    0.1250        14\n",
      "                4.1 supporting mistreatment of individual women     1.0000    0.2381    0.3846        21\n",
      "4.2 supporting systemic discrimination against women as a group     0.6102    0.4932    0.5455        73\n",
      "\n",
      "                                                       accuracy                         0.5361       970\n",
      "                                                      macro avg     0.5993    0.4043    0.4379       970\n",
      "                                                   weighted avg     0.5546    0.5361    0.5274       970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_predict = class_mapper.predict(train_texts)\n",
    "dev_predict = class_mapper.predict(dev_texts)\n",
    "test_predict = class_mapper.predict(test_texts)\n",
    "\n",
    "print(\"TRAIN\"+\"-\"*150)\n",
    "print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ac04470b-39d5-4c54-8ac9-f309c5921f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6fc011c21f4e23af4254e019a72900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed1a8111fde40d5b1572b01b7ddbd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7dcb23ea114f119a82b69127216bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42a0e6722e244c991920841cce7040f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# sbert = SentenceTransformer(\"sentence-transformers/nli-mpnet-base-v2\")\n",
    "sbert = SentenceTransformer(\"all-roberta-large-v1\")\n",
    "# sbert=SentenceTransformer(\"flax-sentence-embeddings/reddit_single-context_mpnet-base\")\n",
    "\n",
    "train_texts_vec = sbert.encode(train_texts, show_progress_bar=True)\n",
    "X_vec = sbert.encode(X, show_progress_bar=True)\n",
    "dev_texts_vec = sbert.encode(dev_texts, show_progress_bar=True)\n",
    "test_texts_vec = sbert.encode(test_texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a3b63451-27a5-4464-95b3-10d53a7ebec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.5714    0.5000    0.5333         8\n",
      "                       1.2 incitement and encouragement of harm     0.7568    0.7778    0.7671        36\n",
      "                                        2.1 descriptive attacks     0.5214    0.5980    0.5571       102\n",
      "                             2.2 aggressive and emotive attacks     0.5876    0.5938    0.5907        96\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4444    0.4138    0.4286        29\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.6556    0.6484    0.6519        91\n",
      "        3.2 immutable gender differences and gender stereotypes     0.4915    0.4833    0.4874        60\n",
      "                            3.3 backhanded gendered compliments     0.3333    0.3333    0.3333         9\n",
      "             3.4 condescending explanations or unwelcome advice     1.0000    0.2857    0.4444         7\n",
      "                4.1 supporting mistreatment of individual women     0.6667    0.3636    0.4706        11\n",
      "4.2 supporting systemic discrimination against women as a group     0.6857    0.6486    0.6667        37\n",
      "\n",
      "                                                       accuracy                         0.5823       486\n",
      "                                                      macro avg     0.6104    0.5133    0.5392       486\n",
      "                                                   weighted avg     0.5888    0.5823    0.5810       486\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.8000    0.2500    0.3810        16\n",
      "                       1.2 incitement and encouragement of harm     0.7143    0.6849    0.6993        73\n",
      "                                        2.1 descriptive attacks     0.5251    0.5610    0.5425       205\n",
      "                             2.2 aggressive and emotive attacks     0.5365    0.5365    0.5365       192\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4091    0.4737    0.4390        57\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.6281    0.6868    0.6562       182\n",
      "        3.2 immutable gender differences and gender stereotypes     0.5042    0.5042    0.5042       119\n",
      "                            3.3 backhanded gendered compliments     0.6667    0.2222    0.3333        18\n",
      "             3.4 condescending explanations or unwelcome advice     0.3333    0.0714    0.1176        14\n",
      "                4.1 supporting mistreatment of individual women     0.6000    0.2857    0.3871        21\n",
      "4.2 supporting systemic discrimination against women as a group     0.5679    0.6301    0.5974        73\n",
      "\n",
      "                                                       accuracy                         0.5577       970\n",
      "                                                      macro avg     0.5714    0.4461    0.4722       970\n",
      "                                                   weighted avg     0.5608    0.5577    0.5517       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100),max_iter=1000)\n",
    "# clf=SVC(C=0.9) # 0.4749 C=0.9\n",
    "clf=SVC(C=0.9)\n",
    "# clf = AdaBoostClassifier()\n",
    "# clf.fit(train_texts_vec, train_labels+dev_labels)\n",
    "clf.fit(X_vec, train_labels)\n",
    "\n",
    "\n",
    "train_predict = clf.predict(train_texts_vec)\n",
    "dev_predict = clf.predict(dev_texts_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "# print(\"TRAIN\"+\"-\"*150)\n",
    "# print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa8560-99ea-4bb8-97b4-fbe180918b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d163f52-15e5-4edb-88b4-f8fd74f05d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
