{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd95532-89f0-4598-9ae3-65be85af7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3398, dev:486, test:970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "df = df[df['label_category'] != 'none']\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Category of Sexism: for posts which are sexist, a four-class classification where systems have to predict one of four categories: (1) threats, (2)  derogation, (3) animosity, (4) prejudiced discussion. \n",
    "\n",
    "Given a post determine the post is belong to which class:\n",
    "1. threats, plans to harm and incitement\n",
    "2. derogation \n",
    "3. animosity\n",
    "4. prejudiced discussions\n",
    "\n",
    "### Post: {POST}\n",
    "### Class: \"\"\"\n",
    "\n",
    "column='label_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd829bbe-1bc6-49a7-987a-adb219ee324d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82141b9a8e04964a64f14cfcc7f6c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d62b107753c42b6a62fc4cf331bb50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ff6d7f36214637ae1460b274402325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3db4329740a4cac80a65526502b5cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0e3013ba47427b8095aab0faf6f1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57093575df74985bee6f11de5c9325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a724616175a443faa53f694c80d14db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc4cafba2374267bc9b6d063bde4ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a6d0255b8c41788b9bc6abc5d708da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9046f0bb4047ddb8a8f4c7d7141bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6dcf8e5fcb41a9b5df975b62b32539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b50b8325bd41cfb3b9a3e77e9521ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, padding_side=\"left\", token=\"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\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=quant_config, device_map={\"\": 0},token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005f83ef-3e53-4b2e-a5a3-9ddaef20839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EDOSDataset(Dataset):\n",
    "    def __init__(self, df, prompt_template, column):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df[column].tolist()\n",
    "        self.prompt_template=prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        inputs, inputs_labels = [], []\n",
    "        for idx in idxs:\n",
    "            \n",
    "            inputs.append(self.prompt_template.replace(\"{POST}\", self.texts[idx]))\n",
    "            inputs_labels.append(self.labels[idx])\n",
    "        \n",
    "        return {\"inputs\":inputs, \"labels\": inputs_labels}\n",
    "    \n",
    "def make_the_generations(model, tokenizer, data_loader):\n",
    "    gen_texts, labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        input_data = batch['inputs']\n",
    "        labels += batch['labels']\n",
    "        tokenized_input_data = tokenizer(input_data, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        # print(tokenized_input_data)\n",
    "        outputs = finetuned_model.generate(\n",
    "            **tokenized_input_data,\n",
    "            pad_token_id= tokenizer.eos_token_id,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(outputs[idx], skip_special_tokens=True)[len(input_data[idx]):]\n",
    "                          for idx in range(len(outputs))]\n",
    "        gen_texts += generated_texts\n",
    "    return gen_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320b01d5-0ba0-453e-83b4-fb7709b93aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [02:49<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = EDOSDataset(df=train_df, prompt_template=prompt_template, column=column)\n",
    "train_dataloader =  DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "train_texts, train_labels = make_the_generations(finetuned_model, tokenizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb597ed0-0967-435b-8cba-403ccd339647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:24<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_data = EDOSDataset(df=dev_df, prompt_template=prompt_template, column=column)\n",
    "dev_dataloader =  DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "dev_texts, dev_labels = make_the_generations(finetuned_model, tokenizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0d642a-188d-4498-8f8d-d3c922f2f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:49<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "test_data = EDOSDataset(df=test_df, prompt_template=prompt_template, column=column)\n",
    "test_dataloader =  DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "test_texts, test_labels = make_the_generations(finetuned_model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f87d528-c337-4c38-a4da-5b3501b205e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              3))),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4)))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              3))),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4)))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vectorizer: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                CountVectorizer(ngram_range=(1, 3))),\n",
       "                               (&#x27;tfidf&#x27;, TfidfVectorizer(ngram_range=(1, 4)))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>count-vec</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 3))</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tfidf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 4))</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Vectorizer',\n",
       "                 FeatureUnion(transformer_list=[('count-vec',\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              3))),\n",
       "                                                ('tfidf',\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4)))])),\n",
       "                ('Classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV, ElasticNetCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "vectorizer_1 = CountVectorizer(ngram_range=(1,3), lowercase=True)\n",
    "vectorizer_2 = TfidfVectorizer( ngram_range=(1,4), \n",
    "                                     lowercase=True, \n",
    "                                     sublinear_tf=False, \n",
    "                                     use_idf=True)\n",
    "features = FeatureUnion([\n",
    "    (\"count-vec\", vectorizer_1),\n",
    "    (\"tfidf\", vectorizer_2),\n",
    "])\n",
    "\n",
    "class_mapper = Pipeline (\n",
    "    steps=[\n",
    "        (\"Vectorizer\", features),\n",
    "        # (\"TruncatedSVD\", TruncatedSVD(n_components=600)),\n",
    "        ('Classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# class_mapper.fit(train_texts+train_labels+dev_texts, train_labels+train_labels+dev_labels)\n",
    "# class_mapper.fit(train_labels, train_labels)\n",
    "class_mapper.fit(train_texts+train_labels, train_labels+train_labels)\n",
    "# +train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4813dd2-6dcb-457b-9627-7cc4c5dc4a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.8000    0.7484    0.7733       310\n",
      "                           2. derogation     0.7936    0.9189    0.8516      1590\n",
      "                            3. animosity     0.8421    0.7734    0.8063      1165\n",
      "               4. prejudiced discussions     0.9492    0.5616    0.7057       333\n",
      "\n",
      "                                accuracy                         0.8184      3398\n",
      "                               macro avg     0.8462    0.7506    0.7842      3398\n",
      "                            weighted avg     0.8260    0.8184    0.8146      3398\n",
      "\n",
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.2973    0.2500    0.2716        44\n",
      "                           2. derogation     0.4965    0.6211    0.5519       227\n",
      "                            3. animosity     0.3571    0.3293    0.3427       167\n",
      "               4. prejudiced discussions     0.4545    0.1042    0.1695        48\n",
      "\n",
      "                                accuracy                         0.4362       486\n",
      "                               macro avg     0.4014    0.3262    0.3339       486\n",
      "                            weighted avg     0.4264    0.4362    0.4168       486\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.2466    0.2022    0.2222        89\n",
      "                           2. derogation     0.4853    0.6167    0.5432       454\n",
      "                            3. animosity     0.3671    0.3153    0.3393       333\n",
      "               4. prejudiced discussions     0.0882    0.0319    0.0469        94\n",
      "\n",
      "                                accuracy                         0.4186       970\n",
      "                               macro avg     0.2968    0.2916    0.2879       970\n",
      "                            weighted avg     0.3843    0.4186    0.3956       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predict = class_mapper.predict(train_texts)\n",
    "dev_predict = class_mapper.predict(dev_texts)\n",
    "test_predict = class_mapper.predict(test_texts)\n",
    "\n",
    "print(\"TRAIN\"+\"-\"*150)\n",
    "print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ac6a5-8a25-4efc-b579-2abf77d33f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
