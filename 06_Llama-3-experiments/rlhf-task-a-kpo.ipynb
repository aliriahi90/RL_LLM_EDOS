{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989a88b6-6b9d-4dd7-a5b3-b2bc1d83af4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd95532-89f0-4598-9ae3-65be85af7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 14000, dev:2000, test:4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "prompt_template = \"\"\"Binary Sexism Detection: A two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist.\n",
    "\n",
    "Given a post determine the post is belong to sexist or not sexist class.\n",
    "\n",
    "### Post: \n",
    "{POST}\n",
    "### Class: \"\"\"\n",
    "\n",
    "column='label_sexist'\n",
    "\n",
    "llm_path = \"task_a_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe9f0f8-2e31-4e32-82c0-394a5e30bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. imports\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, set_seed\n",
    "\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c782b2-f4d5-4dd2-af9d-ae646a13f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The arguments for the DPO training script.\n",
    "    \"\"\"\n",
    "\n",
    "    # data parameters\n",
    "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
    "\n",
    "    # training parameters\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=f\"{llm_path}/\",\n",
    "        metadata={\"help\": \"the location of the SFT model name or path\"},\n",
    "    )\n",
    "    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"optimizer learning rate\"})\n",
    "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
    "    warmup_steps: Optional[int] = field(default=200, metadata={\"help\": \"the number of warmup steps\"})\n",
    "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
    "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
    "\n",
    "    per_device_train_batch_size: Optional[int] = field(default=16, metadata={\"help\": \"train batch size per device\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"eval batch size per device\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    gradient_checkpointing_use_reentrant: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": \"whether to use reentrant for gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
    "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
    "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
    "\n",
    "    max_prompt_length: Optional[int] = field(default=512, metadata={\"help\": \"the maximum prompt length\"})\n",
    "    max_length: Optional[int] = field(default=600, metadata={\"help\": \"the maximum sequence length\"})\n",
    "    max_steps: Optional[int] = field(default=2000, metadata={\"help\": \"max number of training steps\"})\n",
    "    logging_steps: Optional[int] = field(default=100, metadata={\"help\": \"the logging frequency\"})\n",
    "    save_steps: Optional[int] = field(default=100, metadata={\"help\": \"the saving frequency\"})\n",
    "    eval_steps: Optional[int] = field(default=100, metadata={\"help\": \"the evaluation frequency\"})\n",
    "\n",
    "    output_dir: Optional[str] = field(default=f\"{llm_path}_rlhf\", metadata={\"help\": \"the output directory\"})\n",
    "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=True, metadata={\"help\": \"whether to load the model in 4bit\"})\n",
    "    model_dtype: Optional[str] = field(\n",
    "        default=\"float16\", metadata={\"help\": \"model_dtype[float16, bfloat16, float] for loading.\"}\n",
    "    )\n",
    "\n",
    "    # instrumentation\n",
    "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
    "    report_to: Optional[str] = field(\n",
    "        default=\"tensorboard\",\n",
    "        metadata={\n",
    "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
    "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
    "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
    "        },\n",
    "    )\n",
    "    # debug argument for distributed training\n",
    "    ignore_bias_buffers: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
    "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
    "        },\n",
    "    )\n",
    "    seed: Optional[int] = field(\n",
    "        default=0, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"}\n",
    "    )\n",
    "    # f: Optional[str] = field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6cc928-6d36-4be9-abb0-ab6b1967c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_exchange_paired(\n",
    "    df: pd,\n",
    "    column: str,\n",
    "    num_proc=24,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts are structured as follows:\n",
    "      \"Question: \" + <prompt> + \"\\n\\nAnswer: \"\n",
    "    \"\"\"\n",
    "    \n",
    "    def return_prompt_and_responses(samples) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"prompt\": samples[\"question\"],\n",
    "            \"chosen\": samples[\"response_j\"],\n",
    "            \"rejected\": samples[\"response_k\"],\n",
    "        }\n",
    "    labels = df[column].tolist()\n",
    "    labels_set = list(set(labels))\n",
    "    posts = df['text'].tolist()\n",
    "    dataset_lists = {'question':[], 'response_j':[], 'response_k':[]}\n",
    "    for post, label in zip(posts, labels):\n",
    "        \n",
    "        \n",
    "        for index in range(len(labels_set)):\n",
    "            if labels_set[index] != label:\n",
    "                dataset_lists['question'] += [prompt_template.replace(\"{POST}\", post)]\n",
    "                dataset_lists['response_j'] += [label]\n",
    "                dataset_lists['response_k'] += [labels_set[index]]\n",
    "        # dataset_lists.append(dataset_list)\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_lists)\n",
    "    \n",
    "    return dataset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=['question', 'response_j', 'response_k'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cc8080-d5fa-452a-98b2-0f3012dcb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptArguments\n",
    "parser = HfArgumentParser((ScriptArguments,) )\n",
    "# parser.add_parser(\"-f\")\n",
    "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "\n",
    "set_seed(script_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ac9eba-a916-4a2b-a5cb-6a6300731b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd246f6385c40068e394efc48df9135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "torch_dtype = torch.float\n",
    "if script_args.model_dtype == \"float16\":\n",
    "    torch_dtype = torch.float16\n",
    "elif script_args.model_dtype == \"bfloat16\":\n",
    "    torch_dtype = torch.bfloat16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    load_in_4bit=script_args.load_in_4bit,\n",
    "    device_map={\"\": Accelerator().local_process_index},\n",
    "    token=\"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\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "if script_args.ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path, token=\"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\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9374d0c5-af5c-4845-9e0f-3d9980ebebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b449a5d6d04486492b3f220adac34d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ccbe51e7c940cc867f1629653c42a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Load the Stack-exchange paired dataset\n",
    "train_dataset = get_stack_exchange_paired(df=train_df, column=column)\n",
    "\n",
    "# 3. Load evaluation dataset\n",
    "eval_dataset = get_stack_exchange_paired(df=dev_df, column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fb9269-692a-4ac9-a07c-f6c74a94a74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 14000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bea6ed-e42d-4007-b28c-e514bf38fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize training arguments:\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    max_steps=script_args.max_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    save_steps=script_args.save_steps,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    # do_eval=True,\n",
    "    # eval_strategy=\"steps\",\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    output_dir=script_args.output_dir,\n",
    "    report_to=script_args.report_to,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    warmup_steps=script_args.warmup_steps,\n",
    "    optim=script_args.optimizer_type,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"dpo_llama2\",\n",
    "    gradient_checkpointing_kwargs=dict(use_reentrant=script_args.gradient_checkpointing_use_reentrant),\n",
    "    seed=script_args.seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61a7e79-0ae0-4d22-8e6a-61e7e149654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=script_args.lora_r,\n",
    "    lora_alpha=script_args.lora_alpha,\n",
    "    lora_dropout=script_args.lora_dropout,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"out_proj\",\n",
    "        \"fc_in\",\n",
    "        \"fc_out\",\n",
    "        \"wte\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67afb17f-c8f2-400c-8db6-f07fc463ed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_prompt_length, max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0226c8219184401fb50dd3997b005d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860ddd65df2c461990204a1f70c2bd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# 5. initialize the DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=script_args.beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    max_prompt_length=script_args.max_prompt_length,\n",
    "    max_length=script_args.max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6caabd46-9cee-4621-8f4f-5376428faf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 1:58:54, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-667143fe-51456b400472a7d40caf972e;c0a81dad-340b-472c-b8d4-6be02dd9d5ca)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714564-350e50981a512a7e48c45e1d;08709b29-6294-41ab-b14e-1e6856edcd06)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-667146c8-3d2a50ad1a8157da00c39e08;b3fb6c2f-a4c9-4910-8451-1b47a7a6d4a4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6671482f-207b3ee577b5717424006c70;4fd5c2a9-2eb2-454d-9ac1-be313fe9fd85)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714994-428212861ac1547c12952e17;0eec8ec3-272e-40d2-acf1-6dd652293006)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714afd-41ef6b146d983482549606e0;2761a4b0-2b18-4461-8ade-e1501b7ec349)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714c65-2d1a6eb95c793b103570fde5;95fc4411-840c-4601-aff6-25d86eb00926)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714dcb-3b69d46c07af3c477b44daf9;46b0ac25-1428-4295-b9ad-7d1ca884e41d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66714f30-14586dc10abea05b4b057ede;418b3065-7fe2-4317-9988-399f05941d26)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715094-484bdd510ffead51123f6fc8;02a14c8e-4265-489a-ac6b-28f699db5f55)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-667151f8-468f86052082701772fad0f0;b7a9e8ae-4b5b-4d4c-bb63-95f2d5d2700a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6671535b-59799df02b7080e541a7e7fa;784cb751-62c3-4b0c-b012-0aa02987bde8)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-667154c0-2b367f02279b71ad635155dd;d88a6982-23a8-46bd-b3b3-bd97f74ce284)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715623-60741de3020c78b9407014b3;076fbe52-096c-4d80-bb01-9f6f2aa9ad85)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715786-6eefe7e5036302a74cc22f83;d744c396-ad0a-4624-b386-977d0542204f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-667158eb-5b94ae014919395c09aa8428;ca060866-29da-4a9d-a633-a39d3ef01e7e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715a50-76e680de45d904380da204c5;2b0331af-924d-4a54-9fc5-838cc06eb9b9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715bb2-0f0a5e566213ad8f49234411;841ca9cb-5a0a-448c-a026-84b35d92d66a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715d15-44fd8d4f1dfe126824226aab;7e9e533a-2043-4f55-98c1-2f01af3bbc82)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715e79-4a2572d823b91c24308f51e5;77cae158-3c62-4569-84b2-d57e6ccbc094)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66715e7c-1f1992c1154b6c8945eeb8ce;cd9350eb-a050-4833-89e5-75ac152d846a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(script_args.output_dir)\n",
    "\n",
    "# output_dir = os.path.join(script_args.output_dir, \"final_checkpoint\")\n",
    "# dpo_trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d4d15-6dff-4245-a220-6d505fcc5e1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bde2a1-747d-4560-9207-9c3524646a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 14000, dev:2000, test:4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "prompt_template = \"\"\"Binary Sexism Detection: A two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist.\n",
    "\n",
    "Given a post determine the post is belong to sexist or not sexist class. \n",
    "\n",
    "### Post: \n",
    "{POST}\n",
    "### Class: \"\"\"\n",
    "\n",
    "column='label_sexist'\n",
    "\n",
    "llm_path = \"task_a_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bd829bbe-1bc6-49a7-987a-adb219ee324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac4f27187d4d46aa790ce903151138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_path = \"task_a_llm_rlhf/checkpoint-1800\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, padding_side='left')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=quant_config, device_map={\"\": 0})\n",
    "# finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "43f6e416-a8e8-4dda-bf4e-f0d49b4efe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EDOSDataset(Dataset):\n",
    "    def __init__(self, df, prompt_template, column):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df[column].tolist()\n",
    "        self.prompt_template=prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        inputs, inputs_labels = [], []\n",
    "        for idx in idxs:\n",
    "            \n",
    "            inputs.append(self.prompt_template.replace(\"{POST}\", self.texts[idx]))\n",
    "            inputs_labels.append(self.labels[idx])\n",
    "        \n",
    "        return {\"inputs\":inputs, \"labels\": inputs_labels}\n",
    "    \n",
    "def make_the_generations(model, tokenizer, data_loader):\n",
    "    gen_texts, labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        input_data = batch['inputs']\n",
    "        labels += batch['labels']\n",
    "        tokenized_input_data = tokenizer(input_data, padding=True, max_length=512, \n",
    "                                         truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        # print(tokenized_input_data)\n",
    "        outputs = finetuned_model.generate(\n",
    "            **tokenized_input_data,\n",
    "            pad_token_id= tokenizer.eos_token_id,\n",
    "            max_new_tokens=10,\n",
    "            # do_sample=True\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(outputs[idx], skip_special_tokens=True)[len(input_data[idx])-1:]\n",
    "                          for idx in range(len(outputs))]\n",
    "        gen_texts += generated_texts\n",
    "    return gen_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "005f83ef-3e53-4b2e-a5a3-9ddaef20839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [03:41<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = EDOSDataset(df=train_df, prompt_template=prompt_template, column=column)\n",
    "train_dataloader =  DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "train_texts, train_labels = make_the_generations(finetuned_model, tokenizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "628c9715-13df-4f95-b208-640ba3ec1e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['  not sexist\\n\\n### Post: \\nJust got back',\n",
       "  '  not sexist 🎸\\n\\n### Post: \\n',\n",
       "  '  not sexist (not a meaningful answer) \\n\\n###',\n",
       "  '  not sexist\\n\\n### Post: \\nA 21',\n",
       "  '  sexist',\n",
       "  '  not sexist\\n\\nThe post is not about sexism,'],\n",
       " ['not sexist',\n",
       "  'not sexist',\n",
       "  'not sexist',\n",
       "  'not sexist',\n",
       "  'sexist',\n",
       "  'not sexist'])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:6], train_labels[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c4d5c02d-6719-49a4-84d6-a9abedd32070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:31<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_data = EDOSDataset(df=dev_df, prompt_template=prompt_template, column=column)\n",
    "dev_dataloader =  DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "dev_texts, dev_labels = make_the_generations(finetuned_model, tokenizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3d6d7e62-943a-480d-96a0-f66e43d34219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "test_data = EDOSDataset(df=test_df, prompt_template=prompt_template, column=column)\n",
    "test_dataloader =  DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "test_texts, test_labels = make_the_generations(finetuned_model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d28079fd-bf29-46af-a5b1-52569ea16374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not sexist', 'sexist', 'not sexist', 'not sexist', 'sexist']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "73b927be-172a-46b0-9123-0eba974b0dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  not sexist',\n",
       " \"  not sexist\\n\\n### Post: \\nI'm going\",\n",
       " '  feminist (with a hint of frustration) \\n\\n###',\n",
       " '  not sexist (no sexist content) \\n\\n### Post',\n",
       " '  not sexist\\n\\n### Post: \\ni love how',\n",
       " '  not sexist 😊\\n\\n### Post: \\nThe',\n",
       " '  not sexist',\n",
       " \"  not sexist\\n\\n### Post: \\nI'm not\",\n",
       " '  not sexist',\n",
       " '  not sexist',\n",
       " '  not sexist',\n",
       " \"  not sexist\\n\\n### Post: \\nI don't\",\n",
       " '  not sexist (a notepad-like entry, not',\n",
       " \"  not sexist\\n\\n### Post: \\nI'm not\",\n",
       " '  not sexist (just a not about a not-too',\n",
       " '  not sexist',\n",
       " '  sexist',\n",
       " '  not sexist (people stopped taking him seriously due to',\n",
       " '  not sexist',\n",
       " '  sexist']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cb43b5dd-6d8b-47b1-8103-815cea88097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-31 {color: black;}#sk-container-id-31 pre{padding: 0;}#sk-container-id-31 div.sk-toggleable {background-color: white;}#sk-container-id-31 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-31 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-31 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-31 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-31 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-31 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-31 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-31 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-31 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-31 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-31 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-31 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-31 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-31 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-31 div.sk-item {position: relative;z-index: 1;}#sk-container-id-31 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-31 div.sk-item::before, #sk-container-id-31 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-31 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-31 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-31 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-31 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-31 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-31 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-31 div.sk-label-container {text-align: center;}#sk-container-id-31 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-31 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-31\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              4))),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4),\n",
       "                                                                 sublinear_tf=True))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression(C=3))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-161\" type=\"checkbox\" ><label for=\"sk-estimator-id-161\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Vectorizer&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              4))),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4),\n",
       "                                                                 sublinear_tf=True))])),\n",
       "                (&#x27;Classifier&#x27;, LogisticRegression(C=3))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-162\" type=\"checkbox\" ><label for=\"sk-estimator-id-162\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vectorizer: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;count-vec&#x27;,\n",
       "                                CountVectorizer(ngram_range=(1, 4))),\n",
       "                               (&#x27;tfidf&#x27;,\n",
       "                                TfidfVectorizer(ngram_range=(1, 4),\n",
       "                                                sublinear_tf=True))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>count-vec</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-163\" type=\"checkbox\" ><label for=\"sk-estimator-id-163\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 4))</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tfidf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-164\" type=\"checkbox\" ><label for=\"sk-estimator-id-164\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 4), sublinear_tf=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-165\" type=\"checkbox\" ><label for=\"sk-estimator-id-165\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=3)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Vectorizer',\n",
       "                 FeatureUnion(transformer_list=[('count-vec',\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              4))),\n",
       "                                                ('tfidf',\n",
       "                                                 TfidfVectorizer(ngram_range=(1,\n",
       "                                                                              4),\n",
       "                                                                 sublinear_tf=True))])),\n",
       "                ('Classifier', LogisticRegression(C=3))])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV, ElasticNetCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "vectorizer_1 = CountVectorizer(ngram_range=(1,4), lowercase=True)\n",
    "vectorizer_2 = TfidfVectorizer( ngram_range=(1,4), \n",
    "                                     lowercase=True, \n",
    "                                     sublinear_tf=True, \n",
    "                                     use_idf=True)\n",
    "features = FeatureUnion([\n",
    "    (\"count-vec\", vectorizer_1),\n",
    "    (\"tfidf\", vectorizer_2),\n",
    "])\n",
    "\n",
    "class_mapper = Pipeline (\n",
    "    steps=[\n",
    "        (\"Vectorizer\", features),\n",
    "        ('Classifier', LogisticRegression(C=3))\n",
    "])\n",
    "\n",
    "# class_mapper.fit(train_texts+train_labels+dev_texts, train_labels+train_labels+dev_labels)\n",
    "# class_mapper.fit(train_labels, train_labels)\n",
    "class_mapper.fit(train_texts+train_df['text'].tolist(), \n",
    "                 train_labels+train_labels)\n",
    "# +train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f6ac7891-13ea-403b-9fc4-5bdd2f3bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.9998    0.9993    0.9996     10602\n",
      "      sexist     0.9979    0.9994    0.9987      3398\n",
      "\n",
      "    accuracy                         0.9994     14000\n",
      "   macro avg     0.9989    0.9994    0.9991     14000\n",
      "weighted avg     0.9994    0.9994    0.9994     14000\n",
      "\n",
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.9151    0.9544    0.9344      1514\n",
      "      sexist     0.8361    0.7243    0.7762       486\n",
      "\n",
      "    accuracy                         0.8985      2000\n",
      "   macro avg     0.8756    0.8394    0.8553      2000\n",
      "weighted avg     0.8959    0.8985    0.8959      2000\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.9225    0.9472    0.9347      3030\n",
      "      sexist     0.8200    0.7515    0.7843       970\n",
      "\n",
      "    accuracy                         0.8998      4000\n",
      "   macro avg     0.8713    0.8494    0.8595      4000\n",
      "weighted avg     0.8977    0.8998    0.8982      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predict = class_mapper.predict(train_texts)\n",
    "dev_predict = class_mapper.predict(dev_texts)\n",
    "test_predict = class_mapper.predict(test_texts)\n",
    "\n",
    "print(\"TRAIN\"+\"-\"*150)\n",
    "print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ac04470b-39d5-4c54-8ac9-f309c5921f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391ad6faaea0492aafcedbf7e620a378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907be570db5541d8ac0c92716333f1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0903bab748e4e93807706793e5511d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# sbert = SentenceTransformer(\"sentence-transformers/nli-mpnet-base-v2\")\n",
    "sbert = SentenceTransformer(\"all-roberta-large-v1\")\n",
    "\n",
    "train_texts_vec = sbert.encode(train_texts, show_progress_bar=True)\n",
    "dev_texts_vec = sbert.encode(dev_texts, show_progress_bar=True)\n",
    "test_texts_vec = sbert.encode(test_texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a3b63451-27a5-4464-95b3-10d53a7ebec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.9191    0.9524    0.9355      1514\n",
      "      sexist     0.8329    0.7387    0.7830       486\n",
      "\n",
      "    accuracy                         0.9005      2000\n",
      "   macro avg     0.8760    0.8456    0.8592      2000\n",
      "weighted avg     0.8981    0.9005    0.8984      2000\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist     0.9269    0.9409    0.9338      3030\n",
      "      sexist     0.8063    0.7680    0.7867       970\n",
      "\n",
      "    accuracy                         0.8990      4000\n",
      "   macro avg     0.8666    0.8545    0.8603      4000\n",
      "weighted avg     0.8976    0.8990    0.8982      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(50,50,50),max_iter=300, verbose=True)\n",
    "clf=LinearSVC(C=1)\n",
    "# # clf = XGBClassifier(tree_method=\"hist\")\n",
    "# # y = train_labels+dev_labels+train_labels\n",
    "\n",
    "clf.fit(train_texts_vec, train_labels)\n",
    "\n",
    "# train_predict = clf.predict(train_texts_vec)\n",
    "dev_predict = clf.predict(dev_texts_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "# print(\"TRAIN\"+\"-\"*150)\n",
    "# print(classification_report(train_labels, train_predict, digits=4))\n",
    "# print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744d4f4-4666-40bf-984d-09a651fc0f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
