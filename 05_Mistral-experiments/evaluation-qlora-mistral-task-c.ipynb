{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd95532-89f0-4598-9ae3-65be85af7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3398, dev:486, test:970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "df = df[df['label_vector'] != 'none']\n",
    "\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Fine-grained Vector of Sexism: for posts which are sexist, an 11-class classification where systems have to predict one of 11 fine-grained vectors.\n",
    "\n",
    "Given a post determine the post is belong to which class:\n",
    "1.1 threats of harm\n",
    "1.2 incitement and encouragement of harm\n",
    "2.1 descriptive attacks\n",
    "2.2 aggressive and emotive attacks\n",
    "2.3 dehumanising attacks & overt sexual objectification\n",
    "3.1 casual use of gendered slurs, profanities, and insults\n",
    "3.2 immutable gender differences and gender stereotypes\n",
    "3.3 backhanded gendered compliments\n",
    "3.4 condescending explanations or unwelcome advice\n",
    "4.1 supporting mistreatment of individual women\n",
    "4.2 supporting systemic discrimination against women as a group\n",
    "\n",
    "### Post: {POST}\n",
    "### Answer: \"\"\"\n",
    "\n",
    "column='label_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd829bbe-1bc6-49a7-987a-adb219ee324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7348006fa374d5abbb2cd2efa4cc7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_path = \"task_c_llm\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=quant_config, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005f83ef-3e53-4b2e-a5a3-9ddaef20839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EDOSDataset(Dataset):\n",
    "    def __init__(self, df, prompt_template, column):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df[column].tolist()\n",
    "        self.prompt_template=prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        inputs, inputs_labels = [], []\n",
    "        for idx in idxs:\n",
    "            \n",
    "            inputs.append(self.prompt_template.replace(\"{POST}\", self.texts[idx]))\n",
    "            inputs_labels.append(self.labels[idx])\n",
    "        \n",
    "        return {\"inputs\":inputs, \"labels\": inputs_labels}\n",
    "    \n",
    "def make_the_generations(model, tokenizer, data_loader):\n",
    "    gen_texts, labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        input_data = batch['inputs']\n",
    "        labels += batch['labels']\n",
    "        tokenized_input_data = tokenizer(input_data, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        # print(tokenized_input_data)\n",
    "        outputs = finetuned_model.generate(\n",
    "            **tokenized_input_data,\n",
    "            pad_token_id= tokenizer.eos_token_id,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(outputs[idx], skip_special_tokens=True)[len(input_data[idx]):]\n",
    "                          for idx in range(len(outputs))]\n",
    "        gen_texts += generated_texts\n",
    "    return gen_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320b01d5-0ba0-453e-83b4-fb7709b93aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [02:00<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = EDOSDataset(df=train_df, prompt_template=prompt_template, column=column)\n",
    "train_dataloader =  DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "train_texts, train_labels = make_the_generations(finetuned_model, tokenizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb597ed0-0967-435b-8cba-403ccd339647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_data = EDOSDataset(df=dev_df, prompt_template=prompt_template, column=column)\n",
    "dev_dataloader =  DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "dev_texts, dev_labels = make_the_generations(finetuned_model, tokenizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0d642a-188d-4498-8f8d-d3c922f2f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:34<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "test_data = EDOSDataset(df=test_df, prompt_template=prompt_template, column=column)\n",
    "test_dataloader =  DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "test_texts, test_labels = make_the_generations(finetuned_model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1d123f-48a0-43f5-b769-c2a0ad3126d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "task_gen_data = {\n",
    "    \"train\": [{\"text\":text, \"label\":label} for text, label in zip(train_texts, train_labels)],\n",
    "    \"dev\": [{\"text\":text, \"label\":label} for text, label in zip(dev_texts, dev_labels)],\n",
    "    \"test\": [{\"text\":text, \"label\":label} for text, label in zip(test_texts, test_labels)]\n",
    "}\n",
    "\n",
    "with open(llm_path+\"_gens.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(task_gen_data, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8dfe486-edc3-4320-a6a2-3f116b120444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a7eb8d166d425086a1c212ce6f4e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e66b4c54724b0986838d753eb614f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0536118ec14e54948d64d07ac883ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc8b579ea64d76a629717bb2d28a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/nli-mpnet-base-v2\")\n",
    "\n",
    "train_texts_vec = sbert.encode(train_texts, show_progress_bar=True)\n",
    "dev_texts_vec = sbert.encode(dev_texts, show_progress_bar=True)\n",
    "test_texts_vec = sbert.encode(test_texts, show_progress_bar=True)\n",
    "\n",
    "train_texts_all_vec = sbert.encode(train_texts+dev_texts, show_progress_bar=True)\n",
    "train_labels_all = train_labels+dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f50e99-29be-4578-b2f4-316cb421469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.8070    0.8214    0.8142        56\n",
      "                       1.2 incitement and encouragement of harm     0.8897    0.9213    0.9052       254\n",
      "                                        2.1 descriptive attacks     0.8762    0.8689    0.8725       717\n",
      "                             2.2 aggressive and emotive attacks     0.9006    0.8752    0.8877       673\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.7768    0.9050    0.8360       200\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.9305    0.8823    0.9057       637\n",
      "        3.2 immutable gender differences and gender stereotypes     0.7809    0.9233    0.8462       417\n",
      "                            3.3 backhanded gendered compliments     0.8209    0.8594    0.8397        64\n",
      "             3.4 condescending explanations or unwelcome advice     0.8649    0.6809    0.7619        47\n",
      "                4.1 supporting mistreatment of individual women     1.0000    0.6933    0.8189        75\n",
      "4.2 supporting systemic discrimination against women as a group     0.9736    0.8566    0.9113       258\n",
      "\n",
      "                                                       accuracy                         0.8770      3398\n",
      "                                                      macro avg     0.8746    0.8443    0.8545      3398\n",
      "                                                   weighted avg     0.8825    0.8770    0.8775      3398\n",
      "\n",
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.5000    0.3750    0.4286         8\n",
      "                       1.2 incitement and encouragement of harm     0.6875    0.6111    0.6471        36\n",
      "                                        2.1 descriptive attacks     0.4921    0.6078    0.5439       102\n",
      "                             2.2 aggressive and emotive attacks     0.5000    0.4479    0.4725        96\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.2727    0.3103    0.2903        29\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.6341    0.5714    0.6012        91\n",
      "        3.2 immutable gender differences and gender stereotypes     0.4268    0.5833    0.4930        60\n",
      "                            3.3 backhanded gendered compliments     0.3000    0.3333    0.3158         9\n",
      "             3.4 condescending explanations or unwelcome advice     0.3333    0.1429    0.2000         7\n",
      "                4.1 supporting mistreatment of individual women     0.0000    0.0000    0.0000        11\n",
      "4.2 supporting systemic discrimination against women as a group     0.7083    0.4595    0.5574        37\n",
      "\n",
      "                                                       accuracy                         0.5082       486\n",
      "                                                      macro avg     0.4414    0.4039    0.4136       486\n",
      "                                                   weighted avg     0.5132    0.5082    0.5044       486\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.4667    0.4375    0.4516        16\n",
      "                       1.2 incitement and encouragement of harm     0.5556    0.6164    0.5844        73\n",
      "                                        2.1 descriptive attacks     0.4292    0.4439    0.4365       205\n",
      "                             2.2 aggressive and emotive attacks     0.4914    0.4479    0.4687       192\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4000    0.4211    0.4103        57\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.5654    0.5934    0.5791       182\n",
      "        3.2 immutable gender differences and gender stereotypes     0.3720    0.5126    0.4311       119\n",
      "                            3.3 backhanded gendered compliments     0.2857    0.2222    0.2500        18\n",
      "             3.4 condescending explanations or unwelcome advice     0.0000    0.0000    0.0000        14\n",
      "                4.1 supporting mistreatment of individual women     0.3750    0.1429    0.2069        21\n",
      "4.2 supporting systemic discrimination against women as a group     0.6667    0.4384    0.5289        73\n",
      "\n",
      "                                                       accuracy                         0.4753       970\n",
      "                                                      macro avg     0.4189    0.3888    0.3952       970\n",
      "                                                   weighted avg     0.4763    0.4753    0.4710       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100),max_iter=1000)\n",
    "clf=LogisticRegression()\n",
    "clf.fit(train_texts_vec, train_labels)\n",
    "\n",
    "train_predict = clf.predict(train_texts_vec)\n",
    "dev_predict = clf.predict(dev_texts_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "print(\"TRAIN\"+\"-\"*150)\n",
    "print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1572a8e2-fb21-4ec4-9648-a55dc441797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN+DEV--------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.7692    0.7812    0.7752        64\n",
      "                       1.2 incitement and encouragement of harm     0.8707    0.8828    0.8767       290\n",
      "                                        2.1 descriptive attacks     0.8277    0.8327    0.8302       819\n",
      "                             2.2 aggressive and emotive attacks     0.8527    0.8283    0.8404       769\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.7222    0.8515    0.7816       229\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.8990    0.8434    0.8703       728\n",
      "        3.2 immutable gender differences and gender stereotypes     0.7336    0.8889    0.8038       477\n",
      "                            3.3 backhanded gendered compliments     0.7532    0.7945    0.7733        73\n",
      "             3.4 condescending explanations or unwelcome advice     0.8462    0.6111    0.7097        54\n",
      "                4.1 supporting mistreatment of individual women     0.9636    0.6163    0.7518        86\n",
      "4.2 supporting systemic discrimination against women as a group     0.9444    0.8068    0.8702       295\n",
      "\n",
      "                                                       accuracy                         0.8342      3884\n",
      "                                                      macro avg     0.8348    0.7943    0.8076      3884\n",
      "                                                   weighted avg     0.8412    0.8342    0.8347      3884\n",
      "\n",
      "TEST--------------------------------------------------\n",
      "                                                                 precision    recall  f1-score   support\n",
      "\n",
      "                                            1.1 threats of harm     0.4286    0.3750    0.4000        16\n",
      "                       1.2 incitement and encouragement of harm     0.5556    0.6164    0.5844        73\n",
      "                                        2.1 descriptive attacks     0.4300    0.4341    0.4320       205\n",
      "                             2.2 aggressive and emotive attacks     0.4804    0.4479    0.4636       192\n",
      "        2.3 dehumanising attacks & overt sexual objectification     0.4000    0.4211    0.4103        57\n",
      "     3.1 casual use of gendered slurs, profanities, and insults     0.5596    0.5934    0.5760       182\n",
      "        3.2 immutable gender differences and gender stereotypes     0.3720    0.5126    0.4311       119\n",
      "                            3.3 backhanded gendered compliments     0.2857    0.2222    0.2500        18\n",
      "             3.4 condescending explanations or unwelcome advice     0.0000    0.0000    0.0000        14\n",
      "                4.1 supporting mistreatment of individual women     0.3750    0.1429    0.2069        21\n",
      "4.2 supporting systemic discrimination against women as a group     0.6667    0.4384    0.5289        73\n",
      "\n",
      "                                                       accuracy                         0.4722       970\n",
      "                                                      macro avg     0.4139    0.3822    0.3894       970\n",
      "                                                   weighted avg     0.4726    0.4722    0.4676       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100),max_iter=50, batch_size=8)\n",
    "clf=LogisticRegression()\n",
    "clf.fit(train_texts_all_vec, train_labels_all)\n",
    "\n",
    "train_all_predict = clf.predict(train_texts_all_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "print(\"TRAIN+DEV\"+\"-\"*50)\n",
    "print(classification_report(train_labels_all, train_all_predict, digits=4))\n",
    "\n",
    "print(\"TEST\"+\"-\"*50)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
