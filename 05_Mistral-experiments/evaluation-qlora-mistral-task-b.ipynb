{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd95532-89f0-4598-9ae3-65be85af7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3398, dev:486, test:970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_aggregated.csv\")\n",
    "df = df[df['label_category'] != 'none']\n",
    "\n",
    "train_df, dev_df, test_df = df[df['split'] == 'train'], df[df['split'] == 'dev'], df[df['split'] == 'test']\n",
    "\n",
    "print(f\"train: {train_df.shape[0]}, dev:{dev_df.shape[0]}, test:{test_df.shape[0]}\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Category of Sexism: for posts which are sexist, a four-class classification where systems have to predict one of four categories: (1) threats, (2)  derogation, (3) animosity, (4) prejudiced discussion. \n",
    "\n",
    "Given a post determine the post is belong to which class:\n",
    "1. threats, plans to harm and incitement\n",
    "2. derogation \n",
    "3. animosity\n",
    "4. prejudiced discussions\n",
    "\n",
    "### Post: {POST}\n",
    "### Answer: \"\"\"\n",
    "\n",
    "column='label_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd829bbe-1bc6-49a7-987a-adb219ee324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = \"task_b_llm\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=quant_config, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c59c02-d657-4118-80f1-53aac35dea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokenizer.encode(\"1. threats, plans to harm and incitement\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f83ef-3e53-4b2e-a5a3-9ddaef20839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EDOSDataset(Dataset):\n",
    "    def __init__(self, df, prompt_template, column):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df[column].tolist()\n",
    "        self.prompt_template=prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        inputs, inputs_labels = [], []\n",
    "        for idx in idxs:\n",
    "            \n",
    "            inputs.append(self.prompt_template.replace(\"{POST}\", self.texts[idx]))\n",
    "            inputs_labels.append(self.labels[idx])\n",
    "        \n",
    "        return {\"inputs\":inputs, \"labels\": inputs_labels}\n",
    "    \n",
    "def make_the_generations(model, tokenizer, data_loader):\n",
    "    gen_texts, labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        input_data = batch['inputs']\n",
    "        labels += batch['labels']\n",
    "        tokenized_input_data = tokenizer(input_data, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        # print(tokenized_input_data)\n",
    "        outputs = finetuned_model.generate(\n",
    "            **tokenized_input_data,\n",
    "            pad_token_id= tokenizer.eos_token_id,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(outputs[idx], skip_special_tokens=True)[len(input_data[idx]):]\n",
    "                          for idx in range(len(outputs))]\n",
    "        gen_texts += generated_texts\n",
    "    return gen_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b01d5-0ba0-453e-83b4-fb7709b93aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = EDOSDataset(df=train_df, prompt_template=prompt_template, column=column)\n",
    "train_dataloader =  DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "train_texts, train_labels = make_the_generations(finetuned_model, tokenizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb597ed0-0967-435b-8cba-403ccd339647",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = EDOSDataset(df=dev_df, prompt_template=prompt_template, column=column)\n",
    "dev_dataloader =  DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "dev_texts, dev_labels = make_the_generations(finetuned_model, tokenizer, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d642a-188d-4498-8f8d-d3c922f2f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = EDOSDataset(df=test_df, prompt_template=prompt_template, column=column)\n",
    "test_dataloader =  DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "test_texts, test_labels = make_the_generations(finetuned_model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d9290-b26e-403b-9d71-29c2b5877179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# task_gen_data = {\n",
    "#     \"train\": [{\"text\":text, \"label\":label} for text, label in zip(train_texts, train_labels)],\n",
    "#     \"dev\": [{\"text\":text, \"label\":label} for text, label in zip(dev_texts, dev_labels)],\n",
    "#     \"test\": [{\"text\":text, \"label\":label} for text, label in zip(test_texts, test_labels)]\n",
    "# }\n",
    "\n",
    "# with open(llm_path+\"_gens.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "#     json.dump(task_gen_data, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e17b6f5-e9d4-4774-b6c2-a0f6d87c9a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f4beb3c995407bbfa2ac71f22b7f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36bd33eac0c4e0996c2f31b06ebd4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54b7f12d19d443e9c2a04cbd399ad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46fbf74818744debc5d58d0a7186136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/nli-mpnet-base-v2\")\n",
    "\n",
    "train_texts_vec = sbert.encode(train_texts, show_progress_bar=True)\n",
    "dev_texts_vec = sbert.encode(dev_texts, show_progress_bar=True)\n",
    "test_texts_vec = sbert.encode(test_texts, show_progress_bar=True)\n",
    "\n",
    "train_texts_all_vec = sbert.encode(train_texts+dev_texts, show_progress_bar=True)\n",
    "train_labels_all = train_labels+dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27b860ea-1a1d-4e87-bdfc-a9b87e08a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.9631    0.9258    0.9441       310\n",
      "                           2. derogation     0.9444    0.9409    0.9427      1590\n",
      "                            3. animosity     0.9288    0.9296    0.9292      1165\n",
      "               4. prejudiced discussions     0.8800    0.9249    0.9019       333\n",
      "\n",
      "                                accuracy                         0.9341      3398\n",
      "                               macro avg     0.9291    0.9303    0.9295      3398\n",
      "                            weighted avg     0.9345    0.9341    0.9342      3398\n",
      "\n",
      "DEV------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.8125    0.5909    0.6842        44\n",
      "                           2. derogation     0.6390    0.6784    0.6581       227\n",
      "                            3. animosity     0.5305    0.5210    0.5257       167\n",
      "               4. prejudiced discussions     0.5306    0.5417    0.5361        48\n",
      "\n",
      "                                accuracy                         0.6029       486\n",
      "                               macro avg     0.6282    0.5830    0.6010       486\n",
      "                            weighted avg     0.6067    0.6029    0.6029       486\n",
      "\n",
      "TEST------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.7143    0.6180    0.6627        89\n",
      "                           2. derogation     0.6526    0.6123    0.6318       454\n",
      "                            3. animosity     0.5270    0.5856    0.5548       333\n",
      "               4. prejudiced discussions     0.4330    0.4468    0.4398        94\n",
      "\n",
      "                                accuracy                         0.5876       970\n",
      "                               macro avg     0.5817    0.5657    0.5723       970\n",
      "                            weighted avg     0.5939    0.5876    0.5896       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100),max_iter=1000)\n",
    "clf=RidgeClassifierCV()\n",
    "clf.fit(train_texts_vec, train_labels)\n",
    "\n",
    "train_predict = clf.predict(train_texts_vec)\n",
    "dev_predict = clf.predict(dev_texts_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "print(\"TRAIN\"+\"-\"*150)\n",
    "print(classification_report(train_labels, train_predict, digits=4))\n",
    "print(\"DEV\"+\"-\"*150)\n",
    "print(classification_report(dev_labels, dev_predict, digits=4))\n",
    "print(\"TEST\"+\"-\"*150)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df309362-884b-4e7e-b1f1-e8987fdbe4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN+DEV--------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.9475    0.9181    0.9326       354\n",
      "                           2. derogation     0.9173    0.9152    0.9163      1817\n",
      "                            3. animosity     0.8926    0.8926    0.8926      1332\n",
      "               4. prejudiced discussions     0.8662    0.9003    0.8829       381\n",
      "\n",
      "                                accuracy                         0.9063      3884\n",
      "                               macro avg     0.9059    0.9066    0.9061      3884\n",
      "                            weighted avg     0.9066    0.9063    0.9064      3884\n",
      "\n",
      "TEST--------------------------------------------------\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "1. threats, plans to harm and incitement     0.7125    0.6404    0.6746        89\n",
      "                           2. derogation     0.6612    0.6189    0.6394       454\n",
      "                            3. animosity     0.5372    0.5856    0.5603       333\n",
      "               4. prejudiced discussions     0.4118    0.4468    0.4286        94\n",
      "\n",
      "                                accuracy                         0.5928       970\n",
      "                               macro avg     0.5807    0.5729    0.5757       970\n",
      "                            weighted avg     0.5992    0.5928    0.5950       970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_texts_all_vec, train_labels_all)\n",
    "\n",
    "train_all_predict = clf.predict(train_texts_all_vec)\n",
    "test_predict = clf.predict(test_texts_vec)\n",
    "\n",
    "print(\"TRAIN+DEV\"+\"-\"*50)\n",
    "print(classification_report(train_labels_all, train_all_predict, digits=4))\n",
    "\n",
    "print(\"TEST\"+\"-\"*50)\n",
    "print(classification_report(test_labels, test_predict, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63cf00-87b3-4d1e-9621-f4f0b6f09cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
